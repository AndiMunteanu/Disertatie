\chapter*{Introduction} 
\addcontentsline{toc}{chapter}{Introduction}

The unsupervised clustering of points remains an essential tasks in data processing and machine learning. Its relevance is underlined by the diversity of domains where clustering plays an important role. One of these relates to the bioinformatics field, where the clustering became essential in the identification process of cell types \cite{Kiselev2019a} in single-cell RNA-seq datasets or the inference of cell trajectories \cite{Saelens2019}.

As the size of the dataset grew to the size of millions of cells \cite{Svensson2020a}, these tasks have become less obvious: the cell annotations has become more reliant on the output of the clustering algorithms. The challenge was then to improve the accuracy of the classification methods as well as their performance and scalability.

One of the earliest clustering techniques is k-means \cite{Lloyd1982}, which gained popularity due to its simplicity and intuitive approach. Its bias on the size and shape of the clusters determined the introduction of other types of clustering such as density-based (DBSCAN \cite{ester1996}), hierarchical, distribution-based (EM \cite{Dempster1977}).

In the last decades the storage of data in graph-based structures has become more relevant \cite{cook2006mining}, as it was able to capture the relationship between observations. Single-cell analysis is one of the domains where graphs are the natural structure to represent the data, as it can keep the information about the relationship between cells. This lead to the introduction of a new type of clustering methods, namely the graph clustering.

While there are many approaches that were proposed to solve this problem, one of the methods that has become to be adopted at a larger scale was the community detection. This state-of-the art of this category of methods is represented by the Louvain \cite{Blondel2008b} algorithm, which takes a two-step approach to perform a greedy optimisation of an objective (also called quality in literature) function that attempts to define a good graph partitioning. The algorithm was furtherly optimised and improved in later variants such as Louvain with multi-level refinement \cite{Rotta2011}, Smart Local Moving \cite{Waltman2013} and Leiden \cite{Traag2019a}.

The limitation that single-cell analysis faced was that its default data representation structure was not a graph, but a matrix that describe the expression levels of the present cells. To address this issue, the PhenoGraph \cite{Levine2015} was introduced and proposed a pipeline of clustering the data in three steps: dimensionality reduction using approximate PCA (for example, the Lanczos bidiagonalization method \cite{Baglama2016IRLBAFP}) or UMAP \cite{mcinnes2018uniform}, graph construction using kNN \cite{Xu2015}, and graph clustering using community detection.

This pipeline was incorporated and implemented in different frameworks used specifically for processing the single-cell data, like Seurat \cite{Hao2021}, Monocle \cite{Cao2019} or SCANPY \cite{Wolf2018}. In this thesis we will compare how the PhenoGraph pipeline was included in the Seurat and Monocle packages and will analyze the main technical sources that lead to divergent results between them.

This analysis raised the question of the pipeline stability when the random seed changes, as most of the algorithms involved contain at least one stochastic component. The instability cause by random seed was issued in previous works. Some of the approaches were to modify the algorithm as in kmeans++ \cite{kmeanspp} or to add noise to data as in the clust-perturb algorithm \cite{STACEY2021}. We propose \verb|ClustAssess|, a R package that provides a pipeline meant to visually guide the user into choosing a configuration of parameters that leads to results where the seed effect is negligible, without performing any changes to the methods or the original data.

The stability is inferred by running the pipeline multiple times with different seeds and compare the results using the Element-Centric Similarity score \cite{Gates2019}. This score is a clustering comparing tool that does not have any bias regarding the size of the clusters, their shape or the problem of matching, like the traditional measurements (such as NMI \cite{McDaid2015} or ARI \cite{Collins1988}). Our package also provides an optimized implementation of the ECS score that scales well on larger datasets.

