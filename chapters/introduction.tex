\chapter*{Introduction} 
\addcontentsline{toc}{chapter}{Introduction}

The unsupervised clustering of points remains an essential tasks in data processing and machine learning. Its relevance is underlined by the diversity of domains where clustering plays an important role. One of these is bioinformatics, where the clustering is essential for the identification process of cell types \cite{Kiselev2019a} in single-cell RNA-seq datasets or for the inference of cell trajectories \cite{Saelens2019}.

As the size of datasets grew to millions of cells \cite{Svensson2020a}, these tasks became less obvious with the cell annotations relying extensively on the output of clustering algorithms. The next challenge was to improve the accuracy of the clustering methods as well as their performance and scalability.

One of the earliest clustering approaches is k-means \cite{Lloyd1982}, which gained popularity due to its simplicity and intuitive approach. Its bias on the size and shape of the clusters determined the introduction of other types of clustering such as density-based (DBSCAN \cite{ester1996}), hierarchical, distribution-based (EM \cite{Dempster1977}).

In the last decades the storage of data in graph-based structures became more relevant \cite{cook2006mining}, as it was able to capture also relationships between observations. Single-cell analyses is one of the domains where graphs are the natural structure for representing the data, as it preserves the information on the relationship between cells. This lead to the introduction of a new type of clustering methods, namely graph clustering.

While many approaches were proposed to address the clustering problem, one of the methods adopted at larger scale was community detection. The state-of-the art of this class of methods is represented by the Louvain \cite{Blondel2008b} algorithm, a two-step approach for performing a greedy optimisation of an objective (also denoted as quality) function that defines a "good" graph partitioning. The algorithm was further optimised and improved in later versions e.g. Louvain with multi-level refinement \cite{Rotta2011}, Smart Local Moving \cite{Waltman2013} and Leiden \cite{Traag2019a}.

The limitation faced by single-cell analyses derived from the default data representation structure which was historically not a graph, but a matrix summarising the expression levels of genes across cells/ samples. To address this, the PhenoGraph \cite{Levine2015} was introduced in tandem with a proposed a pipeline of clustering the data in three steps: dimensionality reduction using approximate PCA (for example, the Lanczos bidiagonalization method \cite{Baglama2016IRLBAFP}) or UMAP \cite{mcinnes2018uniform}, graph construction using kNN \cite{Xu2015}, and graph clustering using community detection \cite{Girvan2002}.

This pipeline was incorporated into various frameworks, used specifically for processing the single-cell data, like Seurat \cite{Hao2021}, Monocle \cite{Cao2019} or SCANPY \cite{Wolf2018}. In this thesis I will compare how the versions of the PhenoGraph pipeline from the Seurat and Monocle packages focusing on the main technical sources that lead to divergent results.

Several analyses questioned the stability of pipelines when changes were observed for different random seeds (most of the algorithmic components contain some stochastic element). The instability caused by random seed was detected and addressed in previous works. Some approaches focused on modifying the clustering algorithm e.g. kmeans++ \cite{kmeanspp} or to add noise to data e.g. clust-perturb algorithm \cite{STACEY2021}. Together with my collaborators, I propose \verb|ClustAssess|, an R package that provides a pipeline meant to visually guide the user into choosing a configuration of parameters that leads to results where the seed effect is negligible, without performing any changes to the methods or the original data.

The stability is assessed by running the pipeline multiple times with different seeds and comparing the results using the Element-Centric Similarity score \cite{Gates2019}. This score is a clustering comparing tool that does not carry any bias on the size of the clusters, their shape or the problem of matching, like the traditional measurements (such as NMI \cite{McDaid2015} or ARI \cite{Collins1988}). \verb|ClustAssess| also provides an optimized implementation of the ECS score that scales well on larger datasets.

